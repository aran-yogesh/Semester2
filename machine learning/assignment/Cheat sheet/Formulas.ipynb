{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "54249b62-2b35-43c5-b839-968592b3bd8e",
   "metadata": {},
   "source": [
    "### Logistic Regression  (Probability Prediction)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "15728cd3-561f-4f96-88b9-6c134330469e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "# Inputs:\n",
    "# w = weights array including bias (w0, w1, w2...)\n",
    "# x = feature array including 1 for bias (1, x1, x2...)\n",
    "\n",
    "def logistic_regression_prob(w, x):\n",
    "    z = np.dot(w, x)   # Dot product between weights and features\n",
    "    prob = 1 / (1 + np.exp(-z)) # Sigmoid function\n",
    "    return prob\n",
    "\n",
    "# Example: logistic_regression_prob([w0,w1,w2], [1,x1,x2])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc305718-c222-4dd9-9f8e-3de949e4f6c0",
   "metadata": {},
   "source": [
    "### Logistic Regression Cost (Cross-Entropy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "da95610a-5860-4169-ab6f-febf1605b185",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Inputs:\n",
    "# y_true = actual labels (numpy array)\n",
    "# y_pred = predicted probabilities (numpy array)\n",
    "\n",
    "def logistic_cost(y_true, y_pred):\n",
    "    m = len(y_true)\n",
    "    cost = -(1/m) * np.sum(y_true * np.log(y_pred+1e-9) + (1 - y_true) * np.log(1 - y_pred+1e-9))\n",
    "    return cost\n",
    "\n",
    "# Example: logistic_cost(y_true=[0,1], y_pred=[0.3,0.7])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af596674-4cfc-49be-ab10-661f66254291",
   "metadata": {},
   "source": [
    "### Gradient Descent Update (Logistic Regression)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "85843d6d-0066-4e6b-b108-167487a58a1a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Inputs:\n",
    "# X = training features (numpy array shape: (m,n))\n",
    "# y = actual labels (numpy array shape: (m,))\n",
    "# lr = learning rate (float)\n",
    "# epochs = number of iterations (int)\n",
    "\n",
    "def logistic_gradient_descent(X, y, lr=0.01, epochs=1000):\n",
    "    m, n = X.shape\n",
    "    w = np.zeros(n) # Initial weights including bias\n",
    "    for _ in range(epochs):\n",
    "        y_pred = 1/(1 + np.exp(-X.dot(w)))\n",
    "        error = y_pred - y\n",
    "        dw = (1/m) * X.T.dot(error)\n",
    "        w -= lr * dw\n",
    "    return w\n",
    "\n",
    "# Example: logistic_gradient_descent(X, y, lr=0.01, epochs=500)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f64e950-97ab-42dd-948a-8dbb7de71612",
   "metadata": {},
   "source": [
    "### L1 Regularization (Lasso Regression)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "4beee9c2-28be-463e-b9bb-c7726a31d5ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import Lasso\n",
    "\n",
    "# Inputs:\n",
    "# X = feature matrix (numpy array)\n",
    "# y = target values (numpy array)\n",
    "# alpha = regularization strength (float)\n",
    "\n",
    "def lasso_regression(X, y, alpha=1.0):\n",
    "    model = Lasso(alpha=alpha)\n",
    "    model.fit(X, y)\n",
    "    return model.coef_, model.intercept_\n",
    "\n",
    "# Example: lasso_regression(X, y, alpha=0.1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf40e9b0-84b8-4ddb-b933-726d838dc0c8",
   "metadata": {},
   "source": [
    "### L2 Regularization (Ridge Regression)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "fdf7980e-786c-4c57-8146-42907f3bd818",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import Ridge\n",
    "\n",
    "# Inputs:\n",
    "# X = feature matrix (numpy array)\n",
    "# y = target values (numpy array)\n",
    "# alpha = regularization strength (float)\n",
    "\n",
    "def ridge_regression(X, y, alpha=1.0):\n",
    "    model = Ridge(alpha=alpha)\n",
    "    model.fit(X, y)\n",
    "    return model.coef_, model.intercept_\n",
    "\n",
    "# Example: ridge_regression(X, y, alpha=0.1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a41d449-fac8-47b4-b899-e3c8d37355ec",
   "metadata": {},
   "source": [
    "### Bias and Variance Calculation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "9c4b39c9-9d4d-4aa2-b6f8-033f2791297e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Inputs:\n",
    "# y_true = actual target values (numpy array)\n",
    "# predictions = list of arrays of predictions from multiple models\n",
    "\n",
    "def bias_variance(y_true, predictions):\n",
    "    avg_predictions = np.mean(predictions, axis=0)\n",
    "    bias_squared = np.mean((avg_predictions - y_true)**2)\n",
    "    variance = np.mean([np.mean((p - avg_predictions)**2) for p in predictions])\n",
    "    mse = bias_squared + variance\n",
    "    return bias_squared, variance, mse\n",
    "\n",
    "# Example: bias_variance(y_true, [model1_preds, model2_preds, model3_preds])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "875d315b-33a5-4275-84db-a808492ebc49",
   "metadata": {},
   "source": [
    "### K-Means Cluster Update"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "c38b46c9-53dc-4893-9c44-9a7a5b484402",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.cluster import KMeans\n",
    "\n",
    "# Inputs:\n",
    "# X = feature matrix (numpy array)\n",
    "# k = number of clusters (int)\n",
    "\n",
    "def kmeans_clusters(X, k=2):\n",
    "    model = KMeans(n_clusters=k, random_state=42, n_init=10)\n",
    "    model.fit(X)\n",
    "    return model.cluster_centers_, model.labels_\n",
    "\n",
    "# Example: kmeans_clusters(X, k=3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "586ade4d-53e5-4ff7-a0df-1b1ec79c553c",
   "metadata": {},
   "source": [
    "### Euclidean (L2), Manhattan (L1), Chebyshev (Lâˆž) Distance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "b3e26305-d4ae-43a2-b457-f2bd0cda4c02",
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.spatial.distance import cdist\n",
    "\n",
    "# Inputs:\n",
    "# X_train = training data (numpy array)\n",
    "# X_test = test data (numpy array)\n",
    "\n",
    "def distance_metrics(X_test, X_train, metric='euclidean'):\n",
    "    return cdist(X_test, X_train, metric=metric)\n",
    "\n",
    "# Examples:\n",
    "# Euclidean: distance_metrics(X_test, X_train, 'euclidean')\n",
    "# Manhattan: distance_metrics(X_test, X_train, 'cityblock')\n",
    "# Chebyshev: distance_metrics(X_test, X_train, 'chebyshev')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "269d3da6-a022-476b-9903-f3373629fc5c",
   "metadata": {},
   "source": [
    "### Cross-validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "d22e3e27-50e1-4a84-9b8f-d7408b30a0b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import cross_val_score\n",
    "\n",
    "# Inputs:\n",
    "# model = sklearn model instance\n",
    "# X = feature matrix (numpy array)\n",
    "# y = target array (numpy array)\n",
    "# cv = number of folds (int)\n",
    "\n",
    "def cross_validation_score(model, X, y, cv=10):\n",
    "    scores = cross_val_score(model, X, y, cv=cv)\n",
    "    return np.mean(scores)\n",
    "\n",
    "# Example:\n",
    "# from sklearn.neighbors import KNeighborsClassifier\n",
    "# model = KNeighborsClassifier(n_neighbors=5)\n",
    "# cross_validation_score(model, X, y, cv=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e299603-603a-481c-8c27-cfaf8fab6eeb",
   "metadata": {},
   "source": [
    "### Regularized Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "64e2a531-b448-4027-96d1-3f1732fe60b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "# Inputs:\n",
    "# X = features (numpy array)\n",
    "# y = labels (numpy array)\n",
    "# C = inverse of regularization strength (float, smaller=stronger)\n",
    "# penalty = 'l1' or 'l2'\n",
    "\n",
    "def logistic_regression_regularized(X, y, C=1.0, penalty='l2'):\n",
    "    model = LogisticRegression(C=C, penalty=penalty, solver='liblinear')\n",
    "    model.fit(X, y)\n",
    "    return model\n",
    "\n",
    "# Example:\n",
    "# logistic_regression_regularized(X, y, C=0.1, penalty='l1')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "state": {},
    "version_major": 2,
    "version_minor": 0
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
